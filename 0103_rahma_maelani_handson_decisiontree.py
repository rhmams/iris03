# -*- coding: utf-8 -*-
"""0103 Rahma Maelani - HandsOn-DecisionTree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oRYNJasngslLqXcaKhITi9Aa3p3vJSRQ

# VSI2J3 - Dasar Ilmu Data
## Hands On & Praktikum Pekan 08 - Klasifikasi dan Regresi menggunakan model Decision Tree

NIM:

Nama:

Kelas:

## Load library yang dibutuhkan
"""

import numpy as np #untuk kelola data dalam bentuk array
import pandas as pd  #untuk kelola data dalam bentuk dataframe

"""## BAGIAN A - KLASIFIKASI DENGAN DECISION TREE
**Sudi Kasus**: Data Iris (menebak jenis bunga: `setosa`/`versicolor`/`virginica`) berdasarkan `panjang petal`, `panjang sepal`, `lebar petal` dan `lebar sepal` (4 fitur)

**Asumsi**: data yang digunakan sudah melalui tahap preprocessing, sudah siap pakai.

### Tahap 1: Load Dataset
"""

from sklearn.datasets import load_iris  #agar bisa meload dataset Iris

#Load dataset Iris
iris= load_iris()
type(iris)

#tampilkan data asli Iris
iris

#perhatikan bahwa datanya sudah bersih, fitur ada di variabel 'data' dan label ada di variabel 'target
#sekarang kita akan gabungkan fitur dan labelnya dalam sebuah dataframe

#dataframe kita isi dengan fitur
df_iris_lengkap= pd.DataFrame(data=iris.data, columns=iris.feature_names)
#lalu dataframe kita lengkapi dengan labelnya
df_iris_lengkap["target"] = iris.target
#tampilkan data_frame yang sudah siap dan lengkap
df_iris_lengkap.head()

df_iris_lengkap.info()

"""###Tahap 2: Tentukan fitur dan labelnya"""

#tentukan fitur, kali ini kita gunakan kolom 1 sampai 4 sebagai fitur
#kita masukkan kolom 1 sampai 4 ke dataframe df_iris_fitur
df_iris_fitur = df_iris_lengkap[df_iris_lengkap.columns[0:4]]

#tampilkan
df_iris_fitur

#tentukan label, masukkan ke dataframe df_iris_label
df_iris_label = df_iris_lengkap['target']

#tampilkan
df_iris_label

df_iris_label.value_counts()

"""###Tahap 3: Bagi dataset menjadi data latih (*training data*) dan data uji (*testing data*)"""

from sklearn.model_selection import train_test_split #agar bisa membagi dataset menjadi data latih dan data uji

#kita bagi dengan rasio 70% data latih dan 30% data uji
# X = data fitur
# Y = data label
X_train, X_test, Y_train, Y_test = train_test_split(df_iris_fitur, df_iris_label, test_size = 0.3, random_state=99)

# coba kita lihat berapa jumlah data latih dan data ujinya
print("Banyak data latih setelah dilakukan Train-Test Split: ", len(X_train))
print("Banyak data uji setelah dilakukan Train-Test Split: ", len(X_test))

"""###Tahap 4: Siapkan classifier dan tentukan variabel/parameter nya"""

# kita import library untuk klasifikasi dengan K-Nearest Neighbor
# from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# 1) Ini adalah classifier pada KNN
# kita tentukan cara perhitungan jarak dan nilai k nya
# classifier = KNeighborsClassifier(metric='manhattan', n_neighbors=5)

# 2) Ini adalah classifier pada Decision Tree
classifier = DecisionTreeClassifier(max_depth=5, min_samples_split=3)

"""###Tahap 5: Lakukan proses training dengan data latih"""

#ingat bahwa tadi kita sudah punya data latihnya
# X_train = data fitur latih
# Y_train = data label latih
classifier.fit(X_train, Y_train)

"""###Tahap 6: Lakukan pengujian dengan data uji"""

#ingat bahwa tadi kita sudah punya data latihnya
# X_test = data fitur uji
hasil_klasifikasi = classifier.predict(X_test)

#sekarang coba kita lihat hasil prediksinya
hasil_klasifikasi

#coba kita bandingkan antara hasil prediksi dengan label yang sesungguhnya
#ingat kita tadi sudah punya variabel yang menyimpan label yg sebenarnya dari data uji
#Y_test = data label data uji yang sebenarnya (kunci jawabannya)
df_hasil_klasifikasi = pd.DataFrame({"prediksi":hasil_klasifikasi, "label_asli":Y_test})

df_hasil_klasifikasi

"""###Tahap 7: Analisis performanya secara manual"""

#akurasi adalah performance metrics yang paling umum
classifier.score(X_test, Y_test)

#namun lebih lengkap jika kita juga menghitung precision, recall dan f1-score
from sklearn.metrics import classification_report
print(classification_report(Y_test, hasil_klasifikasi))
#array(['setosa', 'versicolor', 'virginica']

#confusion matrix bisa digunakan untuk analisis performa klasifikasi lebih detail lagi
import matplotlib.pyplot as plt # ini untuk visualisasi grafik saja
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(classifier, X_test, Y_test)
#plot_confusion_matrix(classifier, X_test, Y_test)
plt.show()

"""#### Menampilkan Plot Tree"""

from sklearn import tree
tree.plot_tree(classifier)

"""###Optimasi: Hyperparameter Tuning dengan GridSearch dan CV"""

#import library yang dibutuhkan
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

# 1) Ini adalah param_grid untuk KNN
# tentukan parameter apa saja yang ingin dicoba dan nilainya masing-masing
# misalnya kita ingin coba nilai k nya ganjil 1 sampai 15,
# dan kita juga ingin mencoba 4 rumus distance yang berbeda yaitu euclidean, manhattan, chebyshev dan minkowksi
# param_grid = [{'n_neighbors':[1,3,5,7,9,11,13,15],
#               'metric':['euclidean','manhattan','chebyshev','minkowski'],
#               'weights': ['uniform', 'distance'],
#               'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}]

# 2) Ini adalah param_grid untuk Decision Tree
# defining parameter range
param_grid = [
    {'max_depth':[5, 10, 15], 'min_samples_split':[0.1, 1.0, 10], 'min_samples_leaf':[0.1, 0.5, 5]}

 ]

# 1) Ini adalah algoritma Hyperparameter Tuning yang digunakan pada KNN, yaitu GridSearchCV
# Siapkan Grid Search nya dengan 5-fold cross validation
# grid = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='accuracy', cv=5, verbose=3)

# 2) Ini adalah algoritma Hyperparameter Tuning yang digunakan pada Decision Tree, yaitu RandomizedSearchCV
classifier = RandomizedSearchCV(DecisionTreeClassifier(), param_grid, scoring='f1', cv=5, refit = True, verbose = 3)

# 1) Ini adalah training pada KNN
# Waktu Training Menggunakan 4 parameter: 5s
#Â latih model
# grid.fit(X_train, Y_train)

# 2) Ini adalah training pada Decision Tree
# fitting the model for grid search
classifier.fit(X_train, Y_train)

# 1) Ini adalah menampilkan parameter terbaik pada KNN
# tampilkan kombinasi parameter apa yang terbaik
# print(grid.best_params_)

# 2) Ini adalah menampilkan parameter terbaik pada Decision Tree
print(classifier.best_params_)

"""### Tahap 8: Siapkan classifier ke 2 dan tentukan variabel/parameternya"""

# 1) Ini adalah implementasi HPO pada KNN
# kita tentukan cara perhitungan jarak dan nilai k nya
# classifier2 = KNeighborsClassifier(metric='euclidean', n_neighbors=7) # ==> Score performance: 0.96
# classifier2 = KNeighborsClassifier(metric='euclidean', n_neighbors=7, algorithm = 'brute', weights='uniform') # ==> Score performance: 0.96

#Tahap 1
#classifier = KNeighborsClassifier(metric='manhattan', n_neighbors=5) ==> Score Performance: 0.96

# 2) Ini adalah implementasi HPO pada Decision Tree
classifier2 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=0.1, min_samples_split=10)

"""### Tahap 9: Lakukan proses training dengan data latih"""

# 1) Ini adalah training pada KNN
# ingat bahwa tadi kita sudah punya data latihnya
# X_train = data fitur latih
# Y_train = data label latih
# classifier2.fit(X_train, Y_train)

# 2) Ini adalah training pada Decision Tree
classifier2.fit(X_train, Y_train)

"""### Tahap 10: Lakukan pengujian dengan data uji"""

# 1) Ini adalah prediksi pada KNN
# ingat bahwa tadi kita sudah punya data latihnya
# X_test = data fitur uji
# hasil_klasifikasi2 = classifier2.predict(X_test)

# 2) Ini adalah prediksi pada Decision Tree
hasil_klasifikasi2 = classifier2.predict(X_test)

# 1) Ini adalah hasil pada KNN
# sekarang coba kita lihat hasil prediksinya
# hasil_klasifikasi2

# 2) Ini adalah hasil pada Decision Tree
hasil_klasifikasi2

# 1) Ini adalah hasil pada KNN
# coba kita bandingkan antara hasil prediksi dengan label yang sesungguhnya
#ingat kita tadi sudah punya variabel yang menyimpan label yg sebenarnya dari data uji
#Y_test = data label data uji yang sebenarnya (kunci jawabannya)
# df_hasil_klasifikasi2 = pd.DataFrame({"prediksi":hasil_klasifikasi2, "label_asli":Y_test})
# df_hasil_klasifikasi2

# 2) Ini adalah hasil perbandingan prediksi dan label asli pada Decision Tree
df_hasil_klasifikasi2 = pd.DataFrame({"prediksi":hasil_klasifikasi2, "label_asli":Y_test})
df_hasil_klasifikasi2

#70, 106

"""### Tahap 11: Analisis performanya secara manual"""

# 1) Ini adalah analisa performasi pada KNN
# akurasi adalah performance metrics yang paling umum
# classifier2.score(X_test, Y_test)

# 2) Ini adalah analisa performansi pada Decision Tree
classifier2.score(X_test, Y_test)

# 1) Ini adalah analisa performasi Precision, Recall, F1-Score pada KNN
# namun lebih lengkap jika kita juga menghitung precision, recall dan f1-score
# from sklearn.metrics import classification_report
# print(classification_report(Y_test, hasil_klasifikasi2))

# 1) Ini adalah analisa performasi Precision, Recall, F1-Score pada Decision Tree
from sklearn.metrics import classification_report
print(classification_report(Y_test, hasil_klasifikasi2))
#array(['setosa', 'versicolor', 'virginica']

# 1) Ini adalah confusion matrix pada KNN
# confusion matrix bisa digunakan untuk analisis performa klasifikasi lebih detail lagi
# import matplotlib.pyplot as plt # ini untuk visualisasi grafik saja
# from sklearn.metrics import ConfusionMatrixDisplay
# ConfusionMatrixDisplay.from_estimator(classifier2, X_test, Y_test)
# plot_confusion_matrix(classifier, X_test, Y_test)
# plt.show()

# 2) Ini adalah confusion matrix pada Decision Tree
import matplotlib.pyplot as plt # ini untuk visualisasi grafik saja
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(classifier2, X_test, Y_test)

"""Kesimpulan

## BAGIAN B - REGRESI DENGAN DECISION TREE
**Sudi Kasus**: Data Perumahan di California (menebak harga rumah) berdasarkan data-data tentang spesifikasi dan lokasi rumah (8 fitur).

**Asumsi**: data yang digunakan sudah melalui tahap preprocessing

### Tahap 1: Load Dataset
"""

#from sklearn.datasets import load_boston
from sklearn.datasets import fetch_california_housing #agar bisa meload dataset California Housing

#Load dataset California Housing
housing = fetch_california_housing()
type(housing)

#tampilkan data asli Housing
housing

#perhatikan bahwa datanya sudah bersih, fitur ada di variabel 'data' dan label ada di variabel 'target
#sekarang kita akan gabungkan fitur dan labelnya dalam sebuah dataframe

#dataframe kita isi dengan fitur
df_housing_lengkap= pd.DataFrame(data=housing.data, columns=housing.feature_names)
#lalu dataframe kita lengkapi dengan labelnya
df_housing_lengkap["target"] = housing.target
#tampilkan data_frame yang sudah siap dan lengkap
df_housing_lengkap.head()

df_housing_lengkap.info()

"""###Tahap 2: Tentukan fitur dan labelnya"""

#tentukan fitur, masukkan ke dataframe df_housing_fitur
#kali ini kita akan gunakan kolom AveRooms, AveBedrms, AveOccup saja
#df_housing_fitur = df_housing_lengkap[['AveRooms', 'AveBedrms', 'AveOccup','MedInc']]

df_housing_fitur = df_housing_lengkap[df_housing_lengkap.columns[0:8]]

#tampilkan
df_housing_fitur

from sklearn import preprocessing
scaler = preprocessing.StandardScaler().fit(df_housing_fitur)
Xbaru = scaler.transform(df_housing_fitur)
#hasil scaling di variabel Xbaru kita pindahkan ke dataframe baru bernama df_housing_fitur_scaled
df_housing_fitur_scaled = pd.DataFrame(Xbaru, columns=df_housing_fitur.columns)
#kita tampilkan isi data setelah scaling
df_housing_fitur_scaled

#tentukan label, masukkan ke dataframe df_housing_label
df_housing_label = df_housing_lengkap['target']

#tampilkan
df_housing_label

"""###Tahap 3: Bagi dataset menjadi data latih (*training data*) dan data uji (*testing data*)"""

from sklearn.model_selection import train_test_split #agar bisa membagi dataset menjadi data latih dan data uji

#kita bagi dengan rasio 70% data latih dan 30% data uji
# X = data fitur
# Y = data label
X_train, X_test, Y_train, Y_test = train_test_split(df_housing_fitur_scaled, df_housing_label, test_size = 0.3, random_state=99)

# coba kita lihat berapa jumlah data latih dan data ujinya
print("Banyak data latih setelah dilakukan Train-Test Split: ", len(X_train))
print("Banyak data uji setelah dilakukan Train-Test Split: ", len(X_test))

"""###Tahap 4: Siapkan Regressor dan tentukan variabel/parameter nya"""

# kita import library untuk regresi dengan K-Nearest Neighbor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor

# 1) Ini adalah variabel regressor pada KNN
# kita tentukan cara perhitungan jarak dan nilai k nya
# regressor = KNeighborsRegressor(metric='manhattan', n_neighbors=9)
# regressor = KNeighborsRegressor(metric='euclidean', n_neighbors=5)

# 2) Ini adalah variabel regressor pada Decision Tree
regressor = DecisionTreeRegressor(max_depth=5, min_samples_split=2)

"""###Tahap 5: Lakukan proses training dengan data latih"""

# 1) Ini adalah training pada KNN
# ingat bahwa tadi kita sudah punya data latihnya
# X_train = data fitur latih
# Y_train = data label latih
# regressor.fit(X_train, Y_train)

# 2) Ini adalah training pada Decision Tree
regressor.fit(X_train, Y_train)

"""###Tahap 6: Lakukan pengujian dengan data uji"""

# 1) Ini adalah prediksi pada KNN
# ingat bahwa tadi kita sudah punya data ujinya
# X_test = data fitur uji
# hasil_regresi = regressor.predict(X_test)

# 2) Ini adalah prediksi pada Decision Tree
hasil_regresi = regressor.predict(X_test)

# 1) Ini adalah hasil pada KNN
# sekarang coba kita lihat hasil prediksinya
# hasil_regresi

# 2) Ini adalah hasil pada Decision Tree
hasil_regresi

# 1) Ini adalah hasil perbandingan pada KNN
# coba kita bandingkan antara hasil prediksi dengan label yang sesungguhnya
# ingat kita tadi sudah punya variabel yang menyimpan label yg sebenarnya dari data uji
# Y_test = data label data uji yang sebenarnya (kunci jawabannya)
# df_hasil_regresi = pd.DataFrame({"prediksi":hasil_regresi, "label_asli":Y_test})

# df_hasil_regresi

# 2) Ini adalah hasil perbandingan pada Decision Tree
df_hasil_regresi = pd.DataFrame({"prediksi":hasil_regresi, "label_asli":Y_test})
df_hasil_regresi

"""###Tahap 7A: Analisis performanya satu per satu"""

# 1) Ini adalah analisa performansi MAE pada KNN
# MAE (mean absolute error)
# print("Mean absolute error: %.2f" % np.mean(np.absolute(Y_test - hasil_regresi)))
# Mean absolute error: 0.41 ==> dengan regressor awal

# 2) Ini adalah analisa performansi MAE pada Decision Tree
print("Mean absolute error: %.2f" % np.mean(np.absolute(Y_test - hasil_regresi)))

# 1) Ini adalah analisa performansi MSE pada KNN
# MSE (mean squared error)
# from sklearn.metrics import mean_squared_error
# print('MSE=', mean_squared_error(Y_test, hasil_regresi))
# MSE= 0.5960811082550183 ==> dengan regressor awal

# 2) Ini adalah analisa performansi MSE pada Decision Tree
from sklearn.metrics import mean_squared_error
print('MSE=', mean_squared_error(Y_test, hasil_regresi))

# 1) Ini adalah analisa performansi RMSE pada KNN
# RMSE (root mean squared error)
# from sklearn.metrics import mean_squared_error
# print('RMSE=', mean_squared_error(Y_test, hasil_regresi))
# RMSE= 0.35531268761853085 ==> dengan regressor awal

# 2) Ini adalah analisa performansi RMSE pada Decision Tree
from sklearn.metrics import mean_squared_error
RMSE = np.sqrt(mean_squared_error(Y_test, hasil_regresi))
print('RMSE =', RMSE)

# 1) Ini adalah analisa performansi R2 (r squared) pada KNN
# from sklearn.metrics import r2_score
# print('R-squared=',r2_score(Y_test, hasil_regresi))
# R-squared= 0.7269357582389471 ==> dengan regressor awal

# 2) Ini adalah analisa performansi R2 (R Squared) pada Decision Tree
from sklearn.metrics import r2_score
print('R-squared=',r2_score(Y_test, hasil_regresi))

fig, ax = plt.subplots()
#ax.scatter(x, y, s=25, c=c, cmap=plt.cm.coolwarm, zorder=10)
ax.text(1, 9.5,'$R^2=$'+str(round(r2_score(Y_test, hasil_regresi),4)), fontsize=12, verticalalignment='top', multialignment='center')
ax.text(1, 9,'$MSE=$'+str(round(mean_squared_error(Y_test, hasil_regresi),4)), fontsize=12, verticalalignment='top', multialignment='center')

ax.set_xlim(xmin=1)
ax.set_ylim(ymin=1)
ax.set_xlim(xmax=10)
ax.set_ylim(ymax=10)

ax.set_xlabel('Actual Value', fontsize=14)
ax.set_ylabel('Predicted Value', fontsize=14)
ax.scatter(Y_test, hasil_regresi, s=100, c=Y_test, cmap='viridis')

lims = [
    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes
    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes
]

# now plot both limits against eachother
ax.plot(lims, lims, 'r--', alpha=0.75, zorder=0)
ax.set_aspect('equal')
ax.set_xlim(lims)
ax.set_ylim(lims)
ax.grid(True, which='both')

xvalue = np.linspace(1,10,10)
print(xvalue)
lsigma = ax.fill_between(xvalue, xvalue+1, xvalue-1, color='blue', alpha=0.3)

# plt.savefig("/content/drive/MyDrive/Colab-DataScience/regressionDecisionTree.png", dpi=350)
plt.show()

"""###Optimasi: Hyperparameter Tuning dengan GridSearch dan CV"""

#import library yang dibutuhkan
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

# 1) Ini adalah param grid HPO pada KNN
# tentukan parameter apa saja yang ingin dicoba dan nilainya masing-masing
# misalnya kita ingin coba nilai k nya ganjil 1 sampai 15,
# dan kita juga ingin mencoba 4 rumus distance yang berbeda yaitu euclidean, manhattan, chebyshev dan minkowksi
# param_grid = [{'n_neighbors':[1,3,5,7,9,11,13,15], 'metric':['euclidean','manhattan','chebyshev','minkowski']}]

# param_grid = [{'n_neighbors':[1,3,5,7,9,11,13,15],
#               'metric':['euclidean','manhattan','chebyshev','minkowski'],
#               'weights': ['uniform', 'distance'],
#               'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}]

# 2) Ini adalah param grid HPO pada Decision Tree
param_grid = [
    {'max_depth':[5, 10, 15], 'min_samples_split':[0.1, 1.0, 10], 'min_samples_leaf':[0.1, 0.5, 5]}
]

# 1) Ini adalah HPO pada KNN
# siapkan Grid Search nya dengan 5-fold cross validation
# grid = GridSearchCV(KNeighborsRegressor(), param_grid, scoring='r2', cv=5, verbose=3)
# grid = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='accuracy', cv=5, verbose=3)

# 2) Ini adalah HPO pada Decision Tree
grid = RandomizedSearchCV(DecisionTreeRegressor(), param_grid, scoring='r2', cv=5, verbose=3)

#Â 1) Ini adalah train pada KNN
# latih model
# Waktu training HPO: 43S
# grid.fit(X_train, Y_train)

# 2) Ini adalah train pada Decision Tree
grid.fit(X_train, Y_train)

# 1) Ini adalah menampilkan best param pada KNN
# tampilkan kombinasi parameter apa yang terbaik
# print(grid.best_params_)
# Eksperimen 1: {'metric': 'manhattan', 'n_neighbors': 9}
# Eksperimen 2: {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}

# 2) Ini adalah menampilkan best param pada Decision Tree
print(grid.best_params_)

"""### Tahap 8: Siapkan regressor ke 2 dan tentukan variabel/parameternya"""

# 1) Ini adalah implementasi HPO pada KNN
# kita tentukan cara perhitungan jarak dan nilai k nya
# regressor = KNeighborsRegressor(metric='manhattan', n_neighbors=9) ==> R2 = 0.72
# regressor = KNeighborsRegressor(metric='euclidean', n_neighbors=5) ==> R2 = 0.6809893623296548
# regressor2 = KNeighborsRegressor(metric='manhattan', n_neighbors=9) # ==> R2 = 0.72, Eksperimen 1
# regressor2 = KNeighborsRegressor(metric='manhattan', n_neighbors=9, algorithm='auto', weights='distance') #==> Eksperimen 2

# 2) Ini adalah implementasi HPO pada Decision Tree
regressor2 = DecisionTreeRegressor(max_depth=10, min_samples_split=10, min_samples_leaf=5)

"""### Tahap 9: Lakukan proses training dengan data latih"""

# 1) Ini adalah train pada KNN
# ingat bahwa tadi kita sudah punya data latihnya
# X_train = data fitur latih
# Y_train = data label latih
# regressor2.fit(X_train, Y_train)

# 2) Ini adalah train pada Decision Tree
regressor2.fit(X_train, Y_train)

"""###Tahap 10: Lakukan pengujian dengan data uji"""

# 1) Ini adalah pengujian pada KNN
# ingat bahwa tadi kita sudah punya data ujinya
# X_test = data fitur uji
# hasil_regresi2 = regressor2.predict(X_test)

# 2) Ini adalah pengujian pada Decision Tree
hasil_regresi2 = regressor2.predict(X_test)

# 1) Ini adalah hasil pada KNN
# sekarang coba kita lihat hasil prediksinya
# hasil_regresi2

# 2) Ini adalah hasil pada Decision Tree
hasil_regresi2

# 1) Ini adalah hasil perbandingan pada KNN
# coba kita bandingkan antara hasil prediksi dengan label yang sesungguhnya
#ingat kita tadi sudah punya variabel yang menyimpan label yg sebenarnya dari data uji
#Y_test = data label data uji yang sebenarnya (kunci jawabannya)
# df_hasil_regresi2 = pd.DataFrame({"prediksi":hasil_regresi2, "label_asli":Y_test})

# df_hasil_regresi2

# 2) Ini adalah hasil perbandingan pada Decision Tree
df_hasil_regresi2 = pd.DataFrame({"prediksi":hasil_regresi2, "label_asli":Y_test})
df_hasil_regresi2

"""###Tahap 11: Analisis performanya satu per satu"""

# 1) Analisa performnasi pada KNN
# MAE (mean absolute error)
# print("Mean absolute error: %.2f" % np.mean(np.absolute(Y_test - hasil_regresi2)))

# Mean absolute error: 0.41 ==> dengan regressor awal
# Eksp 1: MAE = 0.41
# Eksp 2: MAE = 0.40

# 2) Analisa performansi MAE pada Decision Tree
print("Mean absolute error: %.2f" % np.mean(np.absolute(Y_test - hasil_regresi2)))

# 1) Analisa performansi MSE pada KNN
# MSE (mean squared error)
# from sklearn.metrics import mean_squared_error
# print('MSE=', mean_squared_error(Y_test, hasil_regresi2))

# MSE= 0.5960811082550183 ==> dengan regressor awal

# 2) Analisa performansi MSE pada Decision Tree

from sklearn.metrics import mean_squared_error
print('MSE=', mean_squared_error(Y_test, hasil_regresi2))

# 1) Analisa performansi RMSE pada KNN
# RMSE (root mean squared error)
# from sklearn.metrics import mean_squared_error
# print('RMSE=', mean_squared_error(Y_test, hasil_regresi2))

# RMSE = np.sqrt(mean_squared_error(Y_test, hasil_regresi2))
# print('RMSE =', RMSE)
#RMSE= 0.35531268761853085 ==> dengan regressor awal

# 2) Analisa performansi RMSE pada Decision Tree
from sklearn.metrics import mean_squared_error
RMSE = np.sqrt(mean_squared_error(Y_test, hasil_regresi2))
print('RMSE =', RMSE)

# 1) Analisa performansi R2 (r squared) pada KNN
# from sklearn.metrics import r2_score
# print('R-squared=',r2_score(Y_test, hasil_regresi2))
# R-squared= 0.7269357582389471 ==> dengan regressor awal

# 2) Analisa performansi R2 pada Decision Tree
from sklearn.metrics import r2_score
print('R-squared=',r2_score(Y_test, hasil_regresi2))

"""### Kesimpulan

1.
"""